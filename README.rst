PredNet in PyTorch
================================
Kenta Tanaka, Manabu Kosaka & Eiji Watanabe, 2021



================================
Overview
================================
The software learns a still image sequence generated by a deep learning algorithm, and generates predicted images.



================================
Test environment
================================
OS: Ubuntu 20.04
Python: 3.8
GPU: Nvidia Titan-RTX

torch: 1.8.0
torchvision: 0.9.0
torchaudio: 0.8.0
CUDA: 11.1


 $ sudo apt install ffmpeg

 $ pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html



================================
Preparing data
================================
Put the target video file "YOUR_VIDEO" in "YOUR_DATA" folder.
Execute the following command to generate still images from the video.

 $ python PredNet/generate_image.py YOUR_DATA/YOUR_VIDEO -d YOUR_DATA

To change the width of the image, use the -w option.

 $ python PredNet/generate_image.py YOUR_DATA/YOUR_VIDEO -d YOUR_DATA -w 160

To change the width of the image, use the -g option.

 $ python PredNet/generate_image.py data/YOUR_VIDEO -d data -w 160 -g 120

"train_list.txt" describing the list of files used for training
and "test_list.txt" describing the list of files used for testing are saved in YOUR_DATA folder.
By default, the latter half of the video will be the test data.

Execute the following command to generate dB data from an audio file.

 $ python generate_spectrum.py wave_to_db rain_1.mp3 --with_image

Execute the following command to generate audio files from dB files in the result folder.

 $ python generate_spectrum.py db_to_wave result --with_image



================================
Training
================================
Execute the following command for training.

 $ python PredNet/main.py -i YOUR_DATA/train_list.txt


The learning models are saved in "models" folder.


If you have multiple "train_list.txt"s,
use sequence_list.txt in which directories of "train_list.txt"s are written as follows:
data1/train_list.txt
data2/train_list.txt
data3/train_list.txt
data4/train_list.txt
....
and then, execute the following command.

 $ python PredNet/main.py -seq sequence_list.txt -g 0


If you train from dB files, execute the following command.

 $ python main.py --channels 2,48,96,192 --size 160,512

MSE will be written in log_t.txt.



================================
Training using an Initial model
================================
Deep learning is highly dependent on initial values.
Reference:
Impact of GPU uncertainty on the training of predictive deep neural networks.
"https://arxiv.org/abs/2109.01451"

If the initial value is fixed, stable learning can be achieved.
$ python PredNet/main.py -i YOUR_DATA/train_list.txt --initmodel models/YOUR_INITIAL_MODEL

56_1945_5000.pth
"in https://doi.org/10.6084/m9.figshare.12318950.v2"
is an initial value for easy reproduction of the snake rotation illusion.



================================
For Deterministic learning
================================
Deep learning is also highly dependent on GPU.
Reference:
Impact of GPU uncertainty on the training of predictive deep neural networks.
"https://arxiv.org/abs/2109.01451"

For deterministic learning, use
"torch.backends.cudnn.enabled = False" command,
(https://pytorch.org/docs/stable/backends.html#torch-backends-cudnn)
and use fixed initial weight model by --initmodel option.

In addition, the GPU used should be fixed to one.



================================
Prediction
================================
Generate predicted frames with the following command.

 $ python PredNet/main.py -i YOUR_DATA/test_list.txt --test --initmodel models/YOUR_MODEL -l NUMBER_OF_INPUT_IMAGES --ext NUMBER_OF_PREDICTED_IMAGES

Predicted images (test_#y_ 0.jpg) of all the images described in "test_list.txt" are generated in "result" folder.
Furthermore, for each length of the input image, images (test_#y_1.jpg, test_#y_2.jpg, ...) corresponding to the number of predicted frames are generated.


MSE will be written in log_p.txt.
Prediction No., layer 0, , layer 1, , layer 2, …..layer N
0, 0.0031956271, 0.35379273, 2.6571014, 55.848083
1, 0.0028550925, 0.5585071, 3.421459, 66.66583
2, 0.0021151432, 0.666777, 3.945024, 72.83869
3, 0.002280742, 0.65848637, 4.119229, 75.80301
…...



================================
Prediction by x only
================================
Default
f(x0)=y0、f(x1,y0)=y1, f(x2,y1)=y2, …

--prediction_by_x_only
f(x0)=y0、f(x0, x1)=y1, f(x0, x1, x2)=y2...



================================
Audio
================================
Preparing audio data
 $ python3 generate_spectrum.py wave_to_db rain_1.mp3 --with_image
 $ python3 generate_spectrum.py wave_to_db rain_1.mp3

Training
 $ python3 main.py --channels 2,48,96,192 --size 160,512

Testing
 $ python3 main.py --channels 2,48,96,192 --size 160,512 --test --initmodel models/<model file name>.pth

To Merge split audio data 
$ python3 generate_spectrum.py wave_to_db data --merge

If skip_size is specified in wave_to_db, the same parameter will be required for merge.
$ python3 generate_spectrum.py wave_to_db data --merge --skip_size 160



================================
Gray scale images and 4ch (RGB+Gray) images
================================
Gray scale training
 $ pyton main.py --channels 1,48,96,192
4ch training
 $ pyton main.py --channels 4,48,96,192
 
Gray scale testing
 $ python main.py --test --images data/train_list.txt --channels 1,48,96,192 --initmodel models/10000.pth
4ch testing
 $ python main.py --test --images data/train_list.txt --channels 4,48,96,192 --initmodel models/10000.pth



================================
UP-DOWN-UP learning
================================
“POSTDICTION” learning
$ python main.py --up_down_up



================================
Options
================================

parser = argparse.ArgumentParser(description='PredNet')
parser.add_argument('--images', '-i', default='data/train_list.txt', help='Path to image list file')
parser.add_argument('--sequences', '-seq', default='', help='Path to sequence list file')
parser.add_argument('--device', '-d', default="", type=str,
                    help='Computational device')
parser.add_argument('--root', '-r', default='.',
                    help='Root directory path of sequence and image files')
parser.add_argument('--initmodel', default='',
                    help='Initialize the model from given file')
parser.add_argument('--size', '-s', default='160,120',
                    help='Size of target images. width,height (pixels)')
parser.add_argument('--channels', '-c', default='3,48,96,192',
                    help='Number of channels on each layers')
parser.add_argument('--offset', '-o', default='0,0',
                    help='Center offset of clipping input image (pixels)')
parser.add_argument('--input_len', '-l', default=20, type=int,
                    help='Input frame length fo extended prediction on test (frames)')
parser.add_argument('--ext', '-e', default=10, type=int,
                    help='Extended prediction on test (frames)')
parser.add_argument('--bprop', default=20, type=int,
                    help='Back propagation length (frames)')
parser.add_argument('--save', default=10000, type=int,
                    help='Period of save model and state (frames)')
parser.add_argument('--period', default=1000000, type=int,
                    help='Period of training (frames)')
parser.add_argument('--test', dest='test', action='store_true')
parser.add_argument('--saveimg', dest='saveimg', action='store_true')
parser.add_argument('--useamp', dest='useamp', action='store_true', help='Flag for using AMP')
parser.add_argument('--lr', default=0.001, type=float,
                    help='Learning rate')
parser.add_argument('--lr_rate', default=0.9, type=float,
                    help='Reduction rate for Step lr scheduler')
parser.add_argument('--min_lr', default=0.0001, type=float,
                    help='Lower bound learning rate for Step lr scheduler')
parser.add_argument('--batchsize', default=1, type=int, help='Input batch size')
parser.add_argument('--shuffle', default=False, type=strtobool, help=' True is enable to sampl data randomly (default: False)')
parser.add_argument('--num_workers', default=0, type=int, help='Num. of dataloader process. (default: num of cpu cores')
parser.add_argument('--tensorboard', dest='tensorboard', action='store_true', help='True is enable to log for Tensorboard')
parser.add_argument('--up_down_up', action='store_true', help='True is enable to cycle up-down-up in order')
parser.set_defaults(test=False)
args = parser.parse_args()



================================
Tensorboard logs
================================
Execute the software with "--tensorboard true" option.
Tensorboard logs will be saved "runs" folder.

Then execute the following command.

 $ python main.py --tensorboard
 $ tensorboard --logdir runs



================================
Images from Tensorboard logs
================================
 $ python ext_tensorboard_imgs.py --path <log_dir_path> --outdir <output_dir_path>

Example:
The image will be output as "date_outputstep.jpg" under the directory of each layer name.

<output_dir_path>
├── Conv_sequential_layer0_time0
│   ├── 2021-07-18_0step.jpg
│   ├── 2021-07-18_10000step.jpg
│   └── 2021-07-18_20000step.jpg
~
~
└── UpdateA_layer2_time9
├── 2021-07-18_0step.jpg
├── 2021-07-18_10000step.jpg
└── 2021-07-18_20000step.jpg



================================
From pth to csv, From csv to pth
================================
from pth to csv

 $ python csv_serializer.py pth_to_csv <path to pth file> -dir <csv_directory>

Sample Code;

 $ python3 csv_serializer.py pth_to_csv model_x.pth -dir model_x_folder

from csv to pth

 $ python3 csv_serializer.py csv_to_pth <output_directory> -dir <csv_directory>

Sample Code;

 $ python3 csv_serializer.py csv_to_pth model_x -dir model_x_folder



================================
image_crop_walker (virtual eye motion)
================================
 $ python3 image_crop_walker.py lena_color.jpg

You can create small sequentially numbered images from large images by random walk. The number of images to be created can be changed with the num_steps option.



================================
Reference
================================
"https://coxlab.github.io/prednet/" [Original PredNet]
"https://github.com/quadjr/PredNet" [Implemented by chainer]
"https://github.com/leido/pytorch-prednet" [Implemented by torch]



================================
Application to the study of the visual system
================================
Illusory Motion Reproduced by Deep Neural Networks Trained for Prediction
https://doi.org/10.3389/fpsyg.2018.00345

